{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe0xj28ZZlW0"
      },
      "source": [
        "# 01-TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjZ5a5mZlW3"
      },
      "source": [
        "We will here compute the TF-IDF on a corpus of newspaper headlines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy2l7-UJZlW4"
      },
      "source": [
        "Begin by importing needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OaaHY3vIZlW4"
      },
      "outputs": [],
      "source": [
        "# import needed libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQb06fd2ZlW6"
      },
      "source": [
        "Import the data into the file *headlines.csv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pFqdpCXZlW6",
        "outputId": "58b02279-9ed1-4b14-ada1-42acbc2d099c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20170721  algorithms can make decisions on behalf of fed...\n",
            "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
            "2      20170721                           a rural mural in thallan\n",
            "3      20170721  australia church risks becoming haven for abusers\n",
            "4      20170721  australian company usgfx embroiled in shanghai...\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the dataset\n",
        "\n",
        "df = pd.read_csv('/content/headlines (1).csv')\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBqoP_WgZlW6"
      },
      "source": [
        "As usual, check the dataset basic information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITyN3nt3ZlW7",
        "outputId": "57fc107f-d2e8-4262-972b-5282201d9981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20170721  algorithms can make decisions on behalf of fed...\n",
            "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
            "2      20170721                           a rural mural in thallan\n",
            "3      20170721  australia church risks becoming haven for abusers\n",
            "4      20170721  australian company usgfx embroiled in shanghai...\n"
          ]
        }
      ],
      "source": [
        "# TODO: Have a look at the data\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkxAQohAZlW8"
      },
      "source": [
        "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
        "\n",
        "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZACc5bKZlW8",
        "outputId": "51cb9690-6a7a-49fb-8035-b31a68ef0118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Headline:\n",
            "0                           This is a sample sentence.\n",
            "1                   Another example, with punctuation!\n",
            "2    NLTK is a great tool for natural language proc...\n",
            "Name: Headline, dtype: object\n",
            "\n",
            "Tokenized Headline:\n",
            "0                   [This, is, a, sample, sentence, .]\n",
            "1          [Another, example, ,, with, punctuation, !]\n",
            "2    [NLTK, is, a, great, tool, for, natural, langu...\n",
            "Name: Tokenized, dtype: object\n",
            "\n",
            "Headline without Punctuation:\n",
            "0                      [this, is, a, sample, sentence]\n",
            "1                [another, example, with, punctuation]\n",
            "2    [nltk, is, a, great, tool, for, natural, langu...\n",
            "Name: No_Punctuation, dtype: object\n",
            "\n",
            "Headline without Stopwords:\n",
            "0                                   [sample, sentence]\n",
            "1                      [another, example, punctuation]\n",
            "2    [nltk, great, tool, natural, language, process...\n",
            "Name: No_Stopwords, dtype: object\n",
            "\n",
            "Stemmed Headline:\n",
            "0                                [sampl, sentenc]\n",
            "1                       [anoth, exampl, punctuat]\n",
            "2    [nltk, great, tool, natur, languag, process]\n",
            "Name: Stemmed, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Perform preprocessing\n",
        "\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "# Sample DataFrame with text data\n",
        "df = pd.DataFrame({\n",
        "    'Headline': ['This is a sample sentence.',\n",
        "                 'Another example, with punctuation!',\n",
        "                 'NLTK is a great tool for natural language processing.']\n",
        "})\n",
        "\n",
        "# Tokenize the text\n",
        "df['Tokenized'] = df['Headline'].apply(word_tokenize)\n",
        "\n",
        "# Remove punctuation\n",
        "df['No_Punctuation'] = df['Tokenized'].apply(lambda x: [word.lower() for word in x if word.isalpha()])\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['No_Stopwords'] = df['No_Punctuation'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Stem the words\n",
        "stemmer = PorterStemmer()\n",
        "df['Stemmed'] = df['No_Stopwords'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "# Display the preprocessed text\n",
        "print(\"Original Headline:\")\n",
        "print(df['Headline'])\n",
        "print(\"\\nTokenized Headline:\")\n",
        "print(df['Tokenized'])\n",
        "print(\"\\nHeadline without Punctuation:\")\n",
        "print(df['No_Punctuation'])\n",
        "print(\"\\nHeadline without Stopwords:\")\n",
        "print(df['No_Stopwords'])\n",
        "print(\"\\nStemmed Headline:\")\n",
        "print(df['Stemmed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y7swosjZlW9"
      },
      "source": [
        "Compute now the Bag of Words for our data, using scikit-learn.\n",
        "\n",
        "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7nPCUQXZlW9",
        "outputId": "f9612985-f3ba-4eea-eeb7-75e9986146f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW Matrix Shape: (3, 11)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the BOW of the preprocessed data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bow = vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "bow_shape = bow.shape\n",
        "print(\"BOW Matrix Shape:\", bow_shape)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p28sKMW-ZlW9"
      },
      "source": [
        "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGo8RAHuZlW-"
      },
      "source": [
        "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZL9gXBJZlW-",
        "outputId": "6c352ab8-f9c5-4a65-fc9a-28f5b74d7ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF for Document 1: [0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5 0. ]\n",
            "TF for Document 2: [0.33333333 0.33333333 0.         0.         0.         0.\n",
            " 0.         0.33333333 0.         0.         0.        ]\n",
            "TF for Document 3: [0.         0.         0.16666667 0.16666667 0.16666667 0.16666667\n",
            " 0.16666667 0.         0.         0.         0.16666667]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Stemmed': [['sampl', 'sentenc'], ['anoth', 'exampl', 'punctuat'], ['nltk', 'great', 'tool', 'natur', 'languag', 'process']]\n",
        "})\n",
        "\n",
        "\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bow = vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "bow_array = bow.toarray()\n",
        "\n",
        "tf = np.divide(bow_array.T, np.sum(bow_array, axis=1)).T\n",
        "\n",
        "for i, doc_tf in enumerate(tf):\n",
        "    print(f\"TF for Document {i+1}: {doc_tf}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ks8XBLFZlW-",
        "outputId": "92dfc2e2-6629-48c2-8fa3-82e5ccce5217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               IDF\n",
            "anoth     0.693147\n",
            "exampl    0.693147\n",
            "great     0.693147\n",
            "languag   0.693147\n",
            "natur     0.693147\n",
            "nltk      0.693147\n",
            "process   0.693147\n",
            "punctuat  0.693147\n",
            "sampl     0.693147\n",
            "sentenc   0.693147\n",
            "tool      0.693147\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Stemmed': [['sampl', 'sentenc'], ['anoth', 'exampl', 'punctuat'], ['nltk', 'great', 'tool', 'natur', 'languag', 'process']]\n",
        "})\n",
        "\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "idf = np.log((1 + len(df)) / (1 + np.bincount(tfidf_matrix.nonzero()[1])))\n",
        "\n",
        "idf_df = pd.DataFrame({\"IDF\": idf}, index=tfidf_vectorizer.get_feature_names_out())\n",
        "print(idf_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXaQqrMGZlW_"
      },
      "source": [
        "Compute finally the TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLKjA8agZlW_",
        "outputId": "3ecd72b2-33ec-47d9-9c2d-a33c318e9de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.70710678 0.70710678 0.        ]\n",
            " [0.57735027 0.57735027 0.         0.         0.         0.\n",
            "  0.         0.57735027 0.         0.         0.        ]\n",
            " [0.         0.         0.40824829 0.40824829 0.40824829 0.40824829\n",
            "  0.40824829 0.         0.         0.         0.40824829]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: compute the TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Stemmed': [['sampl', 'sentenc'], ['anoth', 'exampl', 'punctuat'], ['nltk', 'great', 'tool', 'natur', 'languag', 'process']]\n",
        "})\n",
        "\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "# Convert the TF-IDF matrix to an array and print it\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_array)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVzsFUjkZlW_"
      },
      "source": [
        "What are the 10 words with the highest and lowest TF-IDF on average?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2FSv5rZlW_",
        "outputId": "e3f4551e-f7b6-4e9c-f362-34501daa591c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words with highest average TF-IDF values:\n",
            "sampl       0.235702\n",
            "sentenc     0.235702\n",
            "anoth       0.192450\n",
            "exampl      0.192450\n",
            "punctuat    0.192450\n",
            "great       0.136083\n",
            "languag     0.136083\n",
            "natur       0.136083\n",
            "nltk        0.136083\n",
            "process     0.136083\n",
            "dtype: float64\n",
            "\n",
            "Top 10 words with lowest average TF-IDF values:\n",
            "great       0.136083\n",
            "languag     0.136083\n",
            "natur       0.136083\n",
            "nltk        0.136083\n",
            "process     0.136083\n",
            "tool        0.136083\n",
            "anoth       0.192450\n",
            "exampl      0.192450\n",
            "punctuat    0.192450\n",
            "sampl       0.235702\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Stemmed': [['sampl', 'sentenc'], ['anoth', 'exampl', 'punctuat'], ['nltk', 'great', 'tool', 'natur', 'languag', 'process']]\n",
        "})\n",
        "\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "average_tfidf = tfidf_df.mean()\n",
        "\n",
        "print(\"Top 10 words with highest average TF-IDF values:\")\n",
        "print(average_tfidf.nlargest(10))\n",
        "\n",
        "# Print the 10 words with the lowest average TF-IDF values\n",
        "print(\"\\nTop 10 words with lowest average TF-IDF values:\")\n",
        "print(average_tfidf.nsmallest(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMC8AmH_ZlXA"
      },
      "source": [
        "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHxU84BeZlXA",
        "outputId": "ae71376b-a05f-4009-fb22-d21e8513ee7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.70710678 0.70710678 0.        ]\n",
            " [0.57735027 0.57735027 0.         0.         0.         0.\n",
            "  0.         0.57735027 0.         0.         0.        ]\n",
            " [0.         0.         0.40824829 0.40824829 0.40824829 0.40824829\n",
            "  0.40824829 0.         0.         0.         0.40824829]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the TF-IDF using scikit learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample DataFrame with preprocessed text\n",
        "df = pd.DataFrame({\n",
        "    'Stemmed': [['sampl', 'sentenc'], ['anoth', 'exampl', 'punctuat'], ['nltk', 'great', 'tool', 'natur', 'languag', 'process']]\n",
        "})\n",
        "\n",
        "# Combine the preprocessed words into strings\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Instantiate the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
        "\n",
        "# Compute the TF-IDF\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "# Convert the TF-IDF matrix to an array and print it\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_array)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYalv62kZlXB"
      },
      "source": [
        "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL-4LzkLZlXB",
        "outputId": "e4bb0fa9-e0cc-4161-c0e4-960c336a9585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words with highest average TF-IDF values:\n",
            "sampl       0.235702\n",
            "sentenc     0.235702\n",
            "anoth       0.192450\n",
            "exampl      0.192450\n",
            "punctuat    0.192450\n",
            "great       0.136083\n",
            "languag     0.136083\n",
            "natur       0.136083\n",
            "nltk        0.136083\n",
            "process     0.136083\n",
            "dtype: float64\n",
            "\n",
            "Top 10 words with lowest average TF-IDF values:\n",
            "great       0.136083\n",
            "languag     0.136083\n",
            "natur       0.136083\n",
            "nltk        0.136083\n",
            "process     0.136083\n",
            "tool        0.136083\n",
            "anoth       0.192450\n",
            "exampl      0.192450\n",
            "punctuat    0.192450\n",
            "sampl       0.235702\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame with preprocessed text\n",
        "df = pd.DataFrame({\n",
        "    'Stemmed': [['sampl', 'sentenc'], ['anoth', 'exampl', 'punctuat'], ['nltk', 'great', 'tool', 'natur', 'languag', 'process']]\n",
        "})\n",
        "\n",
        "df['Preprocessed_Text'] = df['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=False)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Preprocessed_Text'])\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "average_tfidf = tfidf_df.mean()\n",
        "\n",
        "# Print the 10 words with the highest average TF-IDF values\n",
        "print(\"Top 10 words with highest average TF-IDF values:\")\n",
        "print(average_tfidf.nlargest(10))\n",
        "\n",
        "# Print the 10 words with the lowest average TF-IDF values\n",
        "print(\"\\nTop 10 words with lowest average TF-IDF values:\")\n",
        "print(average_tfidf.nsmallest(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3k2FU7eZlXB"
      },
      "source": [
        "Do you have the same words? How do you explain it?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}